{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBTL6vQbNxWtnqqb9JtX0O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aryan-at-ul/image_segmentation/blob/main/pneumonia_image_as_graph_classification_resnet_based.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4TWA3mGD1S4"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install imageio==2.22.4\n",
        "!pip install llvmlite==0.39.1\n",
        "!pip install matplotlib==3.6.2\n",
        "!pip install networkx==2.8.8\n",
        "!pip install numba==0.56.4\n",
        "!pip install numpy\n",
        "!pip install opencv-python==4.6.0.66\n",
        "!pip install psutil==5.9.4\n",
        "!pip install pytz==2022.6\n",
        "!pip install scikit-image==0.19.3\n",
        "!pip install scipy \n",
        "!pip install timm==0.6.12\n",
        "!pip install torch==1.13.0\n",
        "!pip install torchinfo==1.7.1\n",
        "!pip install torchvision==0.14.0\n",
        "# tqdm @ file:///opt/conda/conda-bld/tqdm_1647339053476/work\n",
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric \n",
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils import erdos_renyi_graph, to_networkx, from_networkx\n",
        "from torch_geometric.loader import DataLoader\n",
        "import os\n",
        "from sklearn.model_selection import KFold\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.nn import Sequential as Seq, Linear, ReLU\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import remove_self_loops, add_self_loops\n",
        "from torch_geometric.nn import GraphConv, TopKPooling, GatedGraphConv, JumpingKnowledge\n",
        "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "from torch_geometric.nn import MLP, DynamicEdgeConv, global_max_pool\n",
        "# import torch.nn.functional as F\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import Dataset,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n",
        "import pickle\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import Planetoid\n",
        "# from torch_geometric.logging import init_wandb, log\n",
        "from torch_geometric.nn import GCNConv\n",
        "import random\n",
        "from torch.nn import Linear, ReLU, Dropout\n",
        "from torch_geometric.nn import Sequential, GCNConv, JumpingKnowledge\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "import torch.nn as nn\n",
        "# from torch.nn import Linear\n",
        "# import torch.nn.functional as F\n",
        "# from torch_geometric.nn import GCNConv\n",
        "# from torch_geometric.nn import global_mean_pool"
      ],
      "metadata": {
        "id": "pcJvDhVKF-VY"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -o train_dataloader.pkl -L 'https://drive.google.com/uc?export=download&confirm=yes&id=1luFl1j_zf07eD_CHrpprXPz1udCIhjUU'\n",
        "!curl -o test_dataloader.pkl -L 'https://drive.google.com/uc?export=download&confirm=yes&id=115kJ5EzpCL7TXLKUotJXuK6ngwcyC84E'\n",
        "!curl -o val_dataloader.pkl -L 'https://drive.google.com/uc?export=download&confirm=yes&id=1SlQqiqT6vAlotmT9xc5PMQdeFLTh_PUa'"
      ],
      "metadata": {
        "id": "lPbfNtU3J2M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!du -sh * "
      ],
      "metadata": {
        "id": "u4Q7zuaqJQvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "curretn_path = os.getcwd()\n",
        "path = f\"{curretn_path}\"\n",
        "\n",
        "embed_dim = 128\n",
        "\n",
        "def load_all_from_one_folder(path,type = 0):\n",
        "    all_files = os.listdir(path)\n",
        "    all_data = []\n",
        "    k = 0\n",
        "    for one_g in all_files:\n",
        "        name = one_g.split(\".\")[0]\n",
        "        G = nx.read_gpickle(f\"{path}/{one_g}\")  \n",
        "\n",
        "        data = from_networkx(G)\n",
        "        if type:\n",
        "            data.y = [1]\n",
        "        else:\n",
        "            data.y = [0]\n",
        "        k+= 1\n",
        "        data.x = torch.Tensor([torch.flatten(val).tolist() for val in data.x])#nx.get_node_attributes(G,'image')\n",
        "        data.name = name\n",
        "        all_data.append(data)\n",
        "    return all_data\n",
        "\n",
        "\n",
        "def permute_array(array):\n",
        "    permuted_array = []\n",
        "    for i in range(len(array)):\n",
        "        permuted_array.append(array[i])\n",
        "    return permuted_array\n",
        "\n",
        "def check_if_a_with_name_exisi(path,name):\n",
        "    all_files = os.listdir(path)\n",
        "    if name in all_files:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def dataloader():\n",
        "    \"\"\"\n",
        "    load train and test data\n",
        "    \"\"\"\n",
        "    print(\"loading data\")\n",
        "    train_dataset, test_dataset, val_dataset = None, None, None\n",
        "\n",
        "    if not check_if_a_with_name_exisi(curretn_path,'train_dataloader.pkl'):\n",
        "\n",
        "        train_normal = load_all_from_one_folder(f\"{path}/train/NORMAL\")\n",
        "        train_pneumonia = load_all_from_one_folder(f\"{path}/train/PNEUMONIA\",1)\n",
        "\n",
        "        test_normal = load_all_from_one_folder(f\"{path}/test/NORMAL\")\n",
        "        test_pneumonia = load_all_from_one_folder(f\"{path}/test/PNEUMONIA\",1)\n",
        "\n",
        "        val_normal = load_all_from_one_folder(f\"{path}/val/NORMAL\")\n",
        "        val_pneumonia = load_all_from_one_folder(f\"{path}/val/PNEUMONIA\",1)\n",
        "\n",
        "\n",
        "        train_data_arr = train_normal + train_pneumonia\n",
        "        test_data_arr = test_normal + test_pneumonia\n",
        "        val_data_arr = val_normal + val_pneumonia\n",
        "        # all_data = permute_array(all_data)\n",
        "        random.shuffle(train_data_arr)\n",
        "        random.shuffle(test_data_arr)\n",
        "        random.shuffle(val_data_arr)\n",
        "        \n",
        "        train_dataset = train_data_arr#all_data[:int(len(all_data)*0.8)]\n",
        "        val_dataset = val_data_arr#all_data[int(len(all_data)*0.8):int(len(all_data)*0.8) + 100]\n",
        "        test_dataset = test_data_arr#all_data[int(len(all_data)*0.8):]\n",
        "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True,drop_last=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False,drop_last=True)\n",
        "        # test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True,drop_last=True)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False,drop_last=True)\n",
        "\n",
        "        with open('train_dataloader.pkl', 'wb') as f:\n",
        "            pickle.dump(train_loader, f)\n",
        "\n",
        "        with open('test_dataloader.pkl', 'wb') as f:\n",
        "            pickle.dump(test_loader, f)\n",
        "        \n",
        "        with open('val_dataloader.pkl', 'wb') as f:\n",
        "            pickle.dump(val_loader, f)\n",
        "    else:\n",
        "        with open('train_dataloader.pkl', 'rb') as f:\n",
        "            train_loader = pickle.load(f)\n",
        "\n",
        "        with open('test_dataloader.pkl', 'rb') as f:\n",
        "            test_loader = pickle.load(f)\n",
        "        \n",
        "        with open('val_dataloader.pkl', 'rb') as f:\n",
        "            val_loader = pickle.load(f)\n",
        "\n",
        "    #dataset_from_dataloader = train_loader.dataset\n",
        "    return train_loader, test_loader, train_loader.dataset, test_loader.dataset, val_loader, val_loader.dataset\n",
        "\n",
        "\n",
        "\n",
        "train_loader, test_loader, train_dataset, test_dataset, val_loader, val_dataset = dataloader()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# train_dataset.data = train_dataset.data.to(device)\n",
        "# val_dataset.data = val_dataset.data.to(device)\n",
        "# test_dataset.data = test_dataset.data.to(device)\n",
        "# datasets = [torch.tensor(train_dataset)\n",
        "# , torch.tensor(val_dataset)]\n",
        "\n",
        "# # Send the datasets to the GPU\n",
        "# for i in range(len(datasets)):\n",
        "#     datasets[i] = datasets[i].to('cuda')\n",
        "\n",
        "# # Concatenate the datasets\n",
        "# dataset = ConcatDataset(datasets)\n",
        "train_dataset = [data.to(device) for data in train_dataset]\n",
        "val_dataset = [data.to(device) for data in val_dataset]\n",
        "test_dataset = [data.to(device) for data in test_dataset]\n",
        "\n",
        "\n",
        "dataset = ConcatDataset([train_dataset, val_dataset])\n",
        "\n",
        "\n",
        "print(\"data loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emKmWPHjK4Ag",
        "outputId": "5d89447c-093d-44d7-c9e9-0b97bcf72de5"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading data\n",
            "data loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GCNConv(512, 256)\n",
        "        self.conv2 = GCNConv(256, 128)\n",
        "        self.conv3 = GCNConv(128,64)\n",
        "        self.conv4 = GCNConv(64, 32)\n",
        "        self.lin1 = Linear(32, 16)\n",
        "        self.lin = Linear(16, 2)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv4(x, edge_index)\n",
        "\n",
        "        x = global_mean_pool(x, batch)  \n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.lin1(x)\n",
        "        x = x.relu()\n",
        "        x = self.lin(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "model = GCN(hidden_channels=512) # based on feature size\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "qTm7eTPJLWgO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64aab9e9-2e76-4b3d-d045-b06c76900e0a"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GCN(\n",
              "  (conv1): GCNConv(512, 256)\n",
              "  (conv2): GCNConv(256, 128)\n",
              "  (conv3): GCNConv(128, 64)\n",
              "  (conv4): GCNConv(64, 32)\n",
              "  (lin1): Linear(in_features=32, out_features=16, bias=True)\n",
              "  (lin): Linear(in_features=16, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay = 0.001)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "FuXxeGraLoUe"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    model.train()\n",
        "\n",
        "    for data in train_loader: \n",
        "        # data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch) \n",
        "        data.y = torch.Tensor(data.y)\n",
        "        data.y = torch.Tensor(torch.flatten(data.y))\n",
        "        data.y = data.y.type(torch.LongTensor)\n",
        "        loss = criterion(out, data.y)\n",
        "        loss.backward()  \n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()  \n",
        "\n",
        "def test(loader):\n",
        "     model.eval()\n",
        "\n",
        "     correct = 0\n",
        "     for data in loader:  \n",
        "        #  data = data.to(device)\n",
        "         out = model(data.x, data.edge_index, data.batch)  \n",
        "         data.y = torch.Tensor(data.y)\n",
        "         pred = out.argmax(dim=1).view(-1,1)  \n",
        "         correct += int((pred.to('cpu') == data.y.to('cpu')).sum())  \n",
        "         acc = correct / len(loader.dataset)\n",
        "        #  if acc > 0.91:\n",
        "        #      torch.save(model.state_dict(), 'model_res_10sp.pt')\n",
        "     return correct / len(loader.dataset) \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9ppE-wHALpU8"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model,device,dataloader,loss_fn,optimizer):\n",
        "    train_loss,train_correct=0.0,0\n",
        "    model.train()\n",
        "    correct = 0.0\n",
        "    ite = 0\n",
        "    for data in dataloader:\n",
        "\n",
        "        # print(data)\n",
        "        ite += 1\n",
        "        out = model(data.x, data.edge_index, data.batch) \n",
        "        data.y = torch.Tensor(data.y)\n",
        "        data.y = torch.Tensor(torch.flatten(data.y))\n",
        "        data.y = data.y.type(torch.LongTensor)\n",
        "        pred = out.argmax(dim=1).view(-1,1)\n",
        "        # cfm = confusion_matrix(data.y,pred)\n",
        "        # print(\"train\",cfm)\n",
        "        # tn, fp, fn, tp = confusion_matrix(data.y, pred).ravel()\n",
        "        # print(f\"tn: {tn}, tp: {tp}, fp : {fp}, fn : {fn}\")\n",
        "        acc = accuracy_score(data.y, pred.cpu())\n",
        "        # print(\"accuracy train :\",acc)\n",
        "        loss = criterion(out.to(device), data.y.to(device))\n",
        "        loss.backward()  \n",
        "        train_loss+=loss\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad() \n",
        "        # train_correct += int((pred.to(device) == data.y).sum()) \n",
        "        correct += acc\n",
        "\n",
        "\n",
        "\n",
        "    return train_loss.item()/ite,correct/ite\n",
        "  \n",
        "def valid_epoch(model,device,dataloader,loss_fn):\n",
        "    valid_loss, val_correct = 0.0, 0\n",
        "    model.eval()\n",
        "    ite = 0\n",
        "    for data in dataloader:\n",
        "        ite += 1\n",
        "        out = model(data.x, data.edge_index, data.batch) \n",
        "        data.y = torch.Tensor(data.y)\n",
        "        data.y = torch.Tensor(torch.flatten(data.y))\n",
        "        data.y = data.y.type(torch.LongTensor)\n",
        "        pred = out.argmax(dim=1).view(-1,1)\n",
        "        # cfm = confusion_matrix(data.y,pred)\n",
        "        # print(cfm)\n",
        "        acc = accuracy_score(data.y, pred.cpu())\n",
        "        loss = criterion(out.to(device), data.y.to(device))\n",
        "        loss.backward() \n",
        "        valid_loss+=loss\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad() \n",
        "        val_correct += acc #int((pred == data.y).sum()) \n",
        "\n",
        "    return valid_loss.item()/ite,val_correct/ite  \n"
      ],
      "metadata": {
        "id": "Hgwap2E87Cbh"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "splits=KFold(n_splits=5,shuffle=True,random_state=42)\n",
        "splits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XjjBKB-7gjG",
        "outputId": "0af1aa7f-815f-43db-8a0a-eb5e1a892f7c"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KFold(n_splits=5, random_state=42, shuffle=True)"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = {'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[]}\n",
        "\n",
        "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
        "\n",
        "    print('Fold {}'.format(fold + 1))\n",
        "    print(len(train_idx),len(val_idx))\n",
        "\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    val_sampler = SubsetRandomSampler(val_idx)\n",
        "    train_loader = DataLoader(dataset, batch_size=64, sampler=train_sampler)\n",
        "    val_loader = DataLoader(dataset, batch_size=64, sampler=val_sampler)\n",
        "    \n",
        "\n",
        "    for epoch in range(10):\n",
        "        train_loss, train_correct=train_epoch(model,device,train_loader,criterion,optimizer)\n",
        "        test_loss, test_correct=valid_epoch(model,device,val_loader,criterion)\n",
        "\n",
        "        train_loss = train_loss \n",
        "        train_acc = train_correct \n",
        "        test_loss = test_loss \n",
        "        test_acc = test_correct \n",
        "        print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Val Acc {:.2f} %\".format(epoch + 1,\n",
        "                                                                                                             10,\n",
        "                                                                                                             train_loss,\n",
        "                                                                                                             test_loss,\n",
        "                                                                                                             train_acc,\n",
        "                                                                                                             test_acc))\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['test_loss'].append(test_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['test_acc'].append(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZO6X7ald7VDa",
        "outputId": "7358f2d7-25d4-40af-f7d2-3038ad26f894"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1\n",
            "4185 1047\n",
            "Epoch:1/10 AVG Training Loss:0.163 AVG Test Loss:0.133 AVG Training Acc 0.94 % AVG Val Acc 0.95 %\n",
            "Epoch:2/10 AVG Training Loss:0.139 AVG Test Loss:0.119 AVG Training Acc 0.95 % AVG Val Acc 0.96 %\n",
            "Epoch:3/10 AVG Training Loss:0.105 AVG Test Loss:0.107 AVG Training Acc 0.97 % AVG Val Acc 0.96 %\n",
            "Epoch:4/10 AVG Training Loss:0.099 AVG Test Loss:0.103 AVG Training Acc 0.97 % AVG Val Acc 0.96 %\n",
            "Epoch:5/10 AVG Training Loss:0.117 AVG Test Loss:0.097 AVG Training Acc 0.96 % AVG Val Acc 0.97 %\n",
            "Epoch:6/10 AVG Training Loss:0.100 AVG Test Loss:0.130 AVG Training Acc 0.97 % AVG Val Acc 0.95 %\n",
            "Epoch:7/10 AVG Training Loss:0.091 AVG Test Loss:0.090 AVG Training Acc 0.97 % AVG Val Acc 0.97 %\n",
            "Epoch:8/10 AVG Training Loss:0.091 AVG Test Loss:0.121 AVG Training Acc 0.97 % AVG Val Acc 0.95 %\n",
            "Epoch:9/10 AVG Training Loss:0.081 AVG Test Loss:0.115 AVG Training Acc 0.97 % AVG Val Acc 0.96 %\n",
            "Epoch:10/10 AVG Training Loss:0.093 AVG Test Loss:0.105 AVG Training Acc 0.97 % AVG Val Acc 0.96 %\n",
            "Fold 2\n",
            "4185 1047\n",
            "Epoch:1/10 AVG Training Loss:0.077 AVG Test Loss:0.057 AVG Training Acc 0.97 % AVG Val Acc 0.98 %\n",
            "Epoch:2/10 AVG Training Loss:0.085 AVG Test Loss:0.051 AVG Training Acc 0.97 % AVG Val Acc 0.98 %\n",
            "Epoch:3/10 AVG Training Loss:0.081 AVG Test Loss:0.067 AVG Training Acc 0.97 % AVG Val Acc 0.97 %\n",
            "Epoch:4/10 AVG Training Loss:0.079 AVG Test Loss:0.066 AVG Training Acc 0.97 % AVG Val Acc 0.98 %\n",
            "Epoch:5/10 AVG Training Loss:0.093 AVG Test Loss:0.094 AVG Training Acc 0.96 % AVG Val Acc 0.96 %\n",
            "Epoch:6/10 AVG Training Loss:0.071 AVG Test Loss:0.054 AVG Training Acc 0.98 % AVG Val Acc 0.98 %\n",
            "Epoch:7/10 AVG Training Loss:0.076 AVG Test Loss:0.058 AVG Training Acc 0.98 % AVG Val Acc 0.98 %\n",
            "Epoch:8/10 AVG Training Loss:0.078 AVG Test Loss:0.072 AVG Training Acc 0.97 % AVG Val Acc 0.97 %\n",
            "Epoch:9/10 AVG Training Loss:0.096 AVG Test Loss:0.055 AVG Training Acc 0.97 % AVG Val Acc 0.97 %\n",
            "Epoch:10/10 AVG Training Loss:0.075 AVG Test Loss:0.054 AVG Training Acc 0.97 % AVG Val Acc 0.98 %\n",
            "Fold 3\n",
            "4186 1046\n",
            "Epoch:1/10 AVG Training Loss:0.060 AVG Test Loss:0.049 AVG Training Acc 0.98 % AVG Val Acc 0.98 %\n",
            "Epoch:2/10 AVG Training Loss:0.066 AVG Test Loss:0.064 AVG Training Acc 0.98 % AVG Val Acc 0.97 %\n",
            "Epoch:3/10 AVG Training Loss:0.063 AVG Test Loss:0.068 AVG Training Acc 0.98 % AVG Val Acc 0.98 %\n",
            "Epoch:4/10 AVG Training Loss:0.066 AVG Test Loss:0.054 AVG Training Acc 0.98 % AVG Val Acc 0.98 %\n",
            "Epoch:5/10 AVG Training Loss:0.058 AVG Test Loss:0.061 AVG Training Acc 0.98 % AVG Val Acc 0.98 %\n",
            "Epoch:6/10 AVG Training Loss:0.057 AVG Test Loss:0.042 AVG Training Acc 0.98 % AVG Val Acc 0.99 %\n",
            "Epoch:7/10 AVG Training Loss:0.064 AVG Test Loss:0.047 AVG Training Acc 0.98 % AVG Val Acc 0.98 %\n",
            "Epoch:8/10 AVG Training Loss:0.049 AVG Test Loss:0.047 AVG Training Acc 0.98 % AVG Val Acc 0.98 %\n",
            "Epoch:9/10 AVG Training Loss:0.055 AVG Test Loss:0.057 AVG Training Acc 0.98 % AVG Val Acc 0.98 %\n",
            "Epoch:10/10 AVG Training Loss:0.076 AVG Test Loss:0.067 AVG Training Acc 0.97 % AVG Val Acc 0.97 %\n",
            "Fold 4\n",
            "4186 1046\n",
            "Epoch:1/10 AVG Training Loss:0.058 AVG Test Loss:0.067 AVG Training Acc 0.98 % AVG Val Acc 0.97 %\n",
            "Epoch:2/10 AVG Training Loss:0.056 AVG Test Loss:0.096 AVG Training Acc 0.98 % AVG Val Acc 0.96 %\n",
            "Epoch:3/10 AVG Training Loss:0.074 AVG Test Loss:0.077 AVG Training Acc 0.97 % AVG Val Acc 0.97 %\n",
            "Epoch:4/10 AVG Training Loss:0.066 AVG Test Loss:0.077 AVG Training Acc 0.98 % AVG Val Acc 0.97 %\n",
            "Epoch:5/10 AVG Training Loss:0.049 AVG Test Loss:0.050 AVG Training Acc 0.98 % AVG Val Acc 0.98 %\n",
            "Epoch:6/10 AVG Training Loss:0.062 AVG Test Loss:0.047 AVG Training Acc 0.98 % AVG Val Acc 0.99 %\n",
            "Epoch:7/10 AVG Training Loss:0.055 AVG Test Loss:0.054 AVG Training Acc 0.98 % AVG Val Acc 0.98 %\n",
            "Epoch:8/10 AVG Training Loss:0.056 AVG Test Loss:0.042 AVG Training Acc 0.98 % AVG Val Acc 0.99 %\n",
            "Epoch:9/10 AVG Training Loss:0.055 AVG Test Loss:0.051 AVG Training Acc 0.98 % AVG Val Acc 0.98 %\n",
            "Epoch:10/10 AVG Training Loss:0.056 AVG Test Loss:0.046 AVG Training Acc 0.98 % AVG Val Acc 0.98 %\n",
            "Fold 5\n",
            "4186 1046\n",
            "Epoch:1/10 AVG Training Loss:0.052 AVG Test Loss:0.035 AVG Training Acc 0.98 % AVG Val Acc 0.99 %\n",
            "Epoch:2/10 AVG Training Loss:0.067 AVG Test Loss:0.080 AVG Training Acc 0.98 % AVG Val Acc 0.97 %\n",
            "Epoch:3/10 AVG Training Loss:0.058 AVG Test Loss:0.062 AVG Training Acc 0.98 % AVG Val Acc 0.98 %\n",
            "Epoch:4/10 AVG Training Loss:0.061 AVG Test Loss:0.052 AVG Training Acc 0.98 % AVG Val Acc 0.98 %\n",
            "Epoch:5/10 AVG Training Loss:0.050 AVG Test Loss:0.039 AVG Training Acc 0.98 % AVG Val Acc 0.99 %\n",
            "Epoch:6/10 AVG Training Loss:0.049 AVG Test Loss:0.035 AVG Training Acc 0.98 % AVG Val Acc 0.99 %\n",
            "Epoch:7/10 AVG Training Loss:0.042 AVG Test Loss:0.030 AVG Training Acc 0.98 % AVG Val Acc 0.99 %\n",
            "Epoch:8/10 AVG Training Loss:0.047 AVG Test Loss:0.030 AVG Training Acc 0.98 % AVG Val Acc 0.99 %\n",
            "Epoch:9/10 AVG Training Loss:0.045 AVG Test Loss:0.045 AVG Training Acc 0.99 % AVG Val Acc 0.98 %\n",
            "Epoch:10/10 AVG Training Loss:0.047 AVG Test Loss:0.037 AVG Training Acc 0.98 % AVG Val Acc 0.98 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_train_loss = np.mean(history['train_loss'])\n",
        "avg_test_loss = np.mean(history['test_loss'])\n",
        "avg_train_acc = np.mean(history['train_acc'])\n",
        "avg_test_acc = np.mean(history['test_acc'])\n",
        "\n",
        "print('Performance of {} fold cross validation'.format(10))\n",
        "print(\"Average Training Loss: {:.4f} \\t Average Test Loss: {:.4f} \\t Average Training Acc: {:.3f} \\t Average VAl Acc: {:.3f}\".format(avg_train_loss,avg_test_loss,avg_train_acc,avg_test_acc))  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efHEwsCIRHoG",
        "outputId": "307136ff-5bf4-4fe4-fd64-0106a43e2a35"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance of 10 fold cross validation\n",
            "Average Training Loss: 0.0722 \t Average Test Loss: 0.0672 \t Average Training Acc: 0.975 \t Average VAl Acc: 0.975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = test(test_loader)\n",
        "print(f'Test Acc: {test_acc:.4f}')\n",
        "print(\"number of paramteres for this model\",sum(p.numel() for p in model.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkJRDvCXL6G1",
        "outputId": "8004162e-0c85-4e82-eb87-d0c02304b6ef"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Acc: 0.8654\n",
            "number of paramteres for this model 175122\n"
          ]
        }
      ]
    }
  ]
}